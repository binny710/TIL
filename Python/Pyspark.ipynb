{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents\n",
    "1. RDD\n",
    "2. DataFrame\n",
    "\n",
    "---\n",
    "\n",
    "#### Tips\n",
    "* `SparkContext`의 instance를 만들어야 실행가능하다.\n",
    "* 참고 사이트\n",
    "  * https://spark.apache.org/docs/latest/index.html#\n",
    "  * http://spark.apache.org/docs/latest/api/python/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(\"pyspark_practice\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Accumulator',\n",
       " 'AccumulatorParam',\n",
       " 'BarrierTaskContext',\n",
       " 'BarrierTaskInfo',\n",
       " 'BasicProfiler',\n",
       " 'Broadcast',\n",
       " 'HiveContext',\n",
       " 'MarshalSerializer',\n",
       " 'PickleSerializer',\n",
       " 'Profiler',\n",
       " 'RDD',\n",
       " 'RDDBarrier',\n",
       " 'Row',\n",
       " 'SQLContext',\n",
       " 'SparkConf',\n",
       " 'SparkContext',\n",
       " 'SparkFiles',\n",
       " 'SparkJobInfo',\n",
       " 'SparkStageInfo',\n",
       " 'StatusTracker',\n",
       " 'StorageLevel',\n",
       " 'TaskContext',\n",
       " '_NoValue',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " '_globals',\n",
       " 'accumulators',\n",
       " 'broadcast',\n",
       " 'cloudpickle',\n",
       " 'conf',\n",
       " 'context',\n",
       " 'copy_func',\n",
       " 'files',\n",
       " 'find_spark_home',\n",
       " 'heapq3',\n",
       " 'java_gateway',\n",
       " 'join',\n",
       " 'keyword_only',\n",
       " 'profiler',\n",
       " 'rdd',\n",
       " 'rddsampler',\n",
       " 'resultiterable',\n",
       " 'serializers',\n",
       " 'shuffle',\n",
       " 'since',\n",
       " 'sql',\n",
       " 'statcounter',\n",
       " 'status',\n",
       " 'storagelevel',\n",
       " 'taskcontext',\n",
       " 'traceback_utils',\n",
       " 'types',\n",
       " 'util',\n",
       " 'version',\n",
       " 'wraps']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(pyspark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Drake', 'Travis Scott', 'Post Malone']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "raw = ['{\"name\":\"Drake\"}','{\"name\":\"Travis Scott\"}','{\"name\":\"Post Malone\"}']\n",
    "for i in raw:\n",
    "    json.loads(i)\n",
    "\n",
    "rdd = sc.parallelize(raw)\n",
    "print(rdd.count())\n",
    "\n",
    "rdd.map(json.loads).map(lambda entry:entry['name']).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SQL에서 읽어오기\n",
    "#raw = sqlContext.read.format(\"parquet\").load(\"{path}\")\n",
    "#df = raw.toDF('id', 'artist_id', 'name', 'external_url')\n",
    "\n",
    "df = spark.read.json(\"examples/src/main/resources/people.json\")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select subset columns\n",
    "df_subset = df.select(df[\"id\"], df[\"danceability\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter Rows\n",
    "df_subset = df_subset.filter(df_subset['danceability']>=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aggregate\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df_subset = df_subset.agg(F.avg(df_subset['danceability']), F.max(df_subset['danceability']))\n",
    "df_subset = df_subset.agg(F.avg(df_subset['danceability']).alias('avg_danceability'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## User define funtion\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "udf1 = udf(lambda e:e.upper())\n",
    "\n",
    "@udf(returnType=BooleanType())\n",
    "def udf2(e):\n",
    "    if e >= 0.06:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "df_filtered = df.filter(udf2(df['danceability']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Join\n",
    "joined = artists.join(df, df['artist_id']==artists['id'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
